% \noindent \textbf{\textit{CC News}} \cite{mackenzie2020cc} is a corpus of News articles, we find that each news sample has title and content correspondingly. Generally, content is an elaboration of title while title is a brief abstract of content. They share some key terms and exactly the same topic, which also means that they should be assigned the same label by the classifier. Also we also notice that each sample's content has internal coherence and consistency. For example, the second paragraph is the progressive narration with regard to the first graph. We use this dataset to fine-tune model serving DA on topic classification task. We download it from huggingface transformers\footnote{\url{https://huggingface.co/datasets/cc_news}}.
% \noindent \textbf{\textit{C4 (realnewslike)}} \cite{raffel2019exploring} is a collection of about 750GB of English-language text sourced from the public Common Crawl web scrape. It is clean and used for pre-training language models and word representations. 
% \noindent \textbf{\textit{Amazon Review}} \cite{zhang2015character}, \textbf{\textit{Yelp Review}} \cite{zhang2015character} and \textbf{\textit{IMDB}} \cite{maas-EtAl:2011:ACL-HLT2011} are typical sentiment corpus consists of reviews on restaurant, movies and Amazon, respectively. They have been annotated into various sentiment polarities and levels. In this paper, we remove these annotations and take them as freely available reviews online. Similarly, we will fine-tune text-to-text models on the combination of the three corpus, where duplicates are removed. The fine-tuned model will serve DA on \textbf{sentiment classification} task. 
% \noindent \textbf{\textit{NatCat}} is constructed from Wikipedia, Stack Exchange and Reddit, with natural category annotation. In Reddit part, corresponding subreddit names are assigned as the pseudo labels of post titles. In Stack Exchange, the authors create document-category pairs by pairing question titles or descriptions with their corresponding subareas. In Wikipedia, documents are annotated with categories by the community contributors, which can be found at the bottom of the page. Different from titles of CC News, these pseudo labels often consist of words or phrases that may be the key points of the documents. Although there are noises, they often share the same topics. Therefore we take the pseudo label as $X_{t}$ and the document as $X_{t}$. 



{\verb|hypothesis|}


\begin{enumerate}
\item[$\bullet$] Recipe A ingredients are: 300g chicken, 60g cheese, 100g potatoes
\item[$\bullet$] Recipe B ingredients are: 100g chicken, 120g cheese, 100g potatoes
\end{enumerate}


\begin{verbatim}
 \newtheorem{theorem}{Theorem}
 \newtheorem{lemma}[theorem]{Lemma}
 \newdefinition{rmk}{Remark}
 \newproof{pf}{Proof}
 \newproof{pot}{Proof of Theorem \ref{thm2}}
\end{verbatim}


\newtheorem{theorem}{Theorem}

\begin{theorem}
For system (8), consensus can be achieved with 
$\|T_{\omega z}$ ...
\begin{eqnarray}\label{10}
....
\end{eqnarray}
\end{theorem}


\begin{verbatim}
 \begin{enumerate}[Step 1.]
  \item This is the first step of the example list.
  \item Obviously this is the second step.
  \item The final step to wind up this example.
 \end{enumerate}
\end{verbatim}





{\verb|AG|}


$\mathbf{F}_{aug}$

$\mathbf{F}_{expand}$


\textsc{0SHOT-TC}



{Politics} &  \itshape{CNNPolitics Morning_Joe A side-by-side poll of what makes Obama the most popular politician in the US shows him as much closer to winning the Electoral College than he was a year ago. From CNN.} \\
{Politics}  & \itshape{The Trump era was about to turn into a nightmare. The administration had the right guy. The right guy was Steve Bannon. That is why I'm very glad he ended up in the White House.} \\


ESPN is making a difference this coming weekend—but I won't mention anything about it because we hate your sport. Last April on Real Soccer Talk, former San Diego Earthquakes player, Cristiano Ronaldo said




\toprule

\midrule 

\midrule
\bottomrule

\begin{figure}
%   \centering
%   \hspace*{-8cm}
        \includesvg[scale=.40]{figs/with_aug.svg}
%   \caption{ studies:}
%   \label{var_sample_aug}
%   \centering
%   \hspace*{8cm}
        \includesvg[scale=.40]{figs/with_exp.svg}
    %\caption{Accuracy averaged among three testbeds by varying number of synthetic samples in the experiment setting of \textbf{with aug} and \textbf{with exp}.}
    \caption{Impact of number of synthetic samples, under setting of left: \textbf{with aug} and right:\textbf{with exp}}
    \label{var_sample_augexp}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[scale=.15]{figs/zsl.jpg}
    \caption{The proposed framework}
    \label{whole_frame}
\end{figure}


\subsection{text-to-text language models}
Like pre-trained LMs, pre-training seq2seq models such as T5 \cite{raffel2019exploring} and BART \cite{lewis2020bart} have shown to improve performance across NLP tasks, such as abstractive summarization, question answering, dialogue and translation. They are also be called text-to-text models. 

Both of T5 and BART are pre-trained by (1) corrupting text with an arbitrary noising function, such as spans of text are replaced by masked tokens, and (2) learning a model to reconstruct(denoise) the original text by predicting the true replacement of corrupted tokens. During pre-training, regeneration loss is optimized using cross-entropy loss between output and the decoder’s output.

These models are particularly effective and versatile when fine-tuned for text generation but also works well for comprehension tasks. The reason behind this is they take the best of both the worlds by combining BERT-like mechanism for encoding the (corrupted) source text(fully visible) and GPT-like mechanism for decoding the target text(causal).
Therefore, they are widely used as backbone models in many academic studies and powerful workhorses in industrial scenarios \cite{liu2020multilingual}. 

In this paper, we incorporate pre-trained seq2seq model into the DA framework, after finetuning it to related unsupervised datasets. We choose BART and T5 as representative seq2seq models with their relatively lower computational cost and commonality in many empirical studies.